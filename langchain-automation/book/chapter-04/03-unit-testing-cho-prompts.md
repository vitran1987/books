# Unit Testing cho Prompts

Rachel Martinez tự hào khi deploy phiên bản mới của AI writing assistant. Cô đã carefully tweak prompt template để make outputs more concise và professional, based on user feedback. Testing manual với vài examples cho thấy improvements rõ rệt. Nhưng hai ngày sau deployment, support tickets tăng vọt. Users complain rằng AI giờ đây refuse generate creative content, chỉ tạo ra bland corporate jargon. Rachel confused—cô đã test thoroughly mà. Sau khi investigate, cô realize rằng những manual tests chỉ cover narrow use cases cô aware of, trong khi real users sử dụng tool cho hundreds of different scenarios. Prompt changes tối ưu cho business emails nhưng lại degrade performance cho marketing copy, creative stories, và casual social media posts. Rollback prompt và refund affected premium users cost công ty $15,000. Lesson learned: treat prompts như code—không change mà không có comprehensive test coverage.

Trong traditional software development, unit tests là foundation của quality assurance. Mỗi function có test cases verify nó behaves correctly với various inputs. Khi code được modified, tests chạy lại để catch regressions. Nhưng với AI systems, nhiều developers treat prompts như configuration files—edit freely mà không verify impact. Đây là mistake lớn. Prompts là core logic của AI application; thay đổi prompt có thể completely change behavior, giống như rewrite business logic trong code.

Best practice là treat prompts exactly như treat code: version control, code review, và comprehensive testing trước khi merge changes. Mỗi prompt template nên có associated test suite—set of inputs và expected outputs defining correct behavior. Khi prompt được modified, test suite chạy lại, và changes chỉ được approve nếu pass tất cả tests (hoặc có explicit justification cho failures if they represent intentional behavior changes).

## Xây dựng Golden Dataset

Foundation của prompt testing là "Golden Dataset"—curated collection of input-output pairs representing desired behavior. Mỗi entry gồm một user query, expected response (hoặc properties của response), và explanation về tại sao đây là good example. Size của dataset có thể từ 50 đến 500+ examples tùy vào complexity của use case. Quality quan trọng hơn quantity—better có 50 carefully chosen examples covering diverse scenarios hơn 500 redundant examples.

Process tạo golden dataset typically bắt đầu với real user data. Analyze logs để identify common query patterns, edge cases, và problematic inputs đã cause issues in past. Mỗi category cần representation trong dataset. Ví dụ, cho writing assistant, categories có thể bao gồm: business emails (formal vs casual), marketing content (ads, blog posts, social media), creative writing (stories, poems, scripts), technical documentation (how-to guides, API docs), và personal communication (invitations, thank you notes, condolences).

Grammarly team chia sẻ approach của họ trong một technical talk năm 2024. Họ maintain golden dataset với 380 entries cho AI writing suggestions feature. Dataset được update quarterly based on analysis của user feedback và support tickets. Mỗi entry được manually reviewed bởi ít nhất hai members của quality team để ensure accuracy. Entries đã outdated hoặc no longer representative được retire. Process này ensure dataset remains relevant và reflects current product expectations.

Expected output trong golden dataset không cần phải exact text match. Với LLMs, deterministic outputs không realistic—cùng input có thể generate slightly different responses mỗi lần. Thay vào đó, define expectations ở higher level. Ví dụ: "response must contain introduction, three main points, và conclusion", "tone must be professional but friendly (sentiment score between 0.6-0.8)", "length between 200-300 words", "must include at least one concrete example", "must not contain jargon or acronyms without explanation".

LangSmith cung cấp infrastructure để manage golden datasets và run evaluations. Bạn có thể upload dataset dưới dạng CSV hoặc JSON, define custom evaluators để check expectations, và schedule regular test runs. UI cho phép visualize results—mỗi test case hiển thị input, actual output, evaluation result (pass/fail), và detailed metrics. Nếu có failures, bạn có thể drill down để xem chính xác aspect nào của response không meet expectations.

## Regression Testing Workflow

Một khi golden dataset established, regression testing trở nên straightforward. Workflow điển hình như sau: Developer muốn improve prompt để handle một specific edge case better. Họ tạo branch mới, modify prompt template, và chạy evaluation suite against golden dataset trong development environment. LangSmith runs prompt với mỗi input trong dataset, collects outputs, và applies evaluators. Results hiển thị overall pass rate cùng với detailed breakdown.

Nếu pass rate là 100%, perfect—change có thể submit for review. Nếu có failures, developer cần investigate. Có hai possibilities: (1) failures là legitimate regressions—prompt change unintentionally broke existing functionality, hoặc (2) failures reflect intentional behavior changes—new prompt behaves differently nhưng new behavior là desirable, và golden dataset cần được updated để reflect new expectations.

Distinguish giữa hai cases này requires human judgment. Developer review failed test cases một cách careful. Nếu new output clearly worse than expected output, đó là regression cần fix. Nếu new output arguably better hoặc simply different without being worse, có thể đó là acceptable trade-off. Trong case sau, developer document reasoning và update golden dataset entry để reflect new expectation. Change này phải được reviewed bởi team lead hoặc PM để ensure alignment với product vision.

Asana AI features team implement "diff-based review" process. Mỗi khi regression test có failures, system tự động generate diff showing expected vs actual outputs side-by-side, highlighted differences. Developer và reviewer cùng xem diff này during code review, discuss whether changes acceptable. Nếu acceptable, reviewer approve both prompt change và dataset update. Nếu không, developer phải iterate thêm. Process này đảm bảo prompt changes được scrutinized carefully, similar to how code changes undergo review.

Frequency của regression testing depends on team velocity và risk tolerance. Minimum nên chạy tests trước mỗi deployment to production. Many teams run tests automatically sau mỗi commit to prompt files, integrated vào CI/CD pipeline. GitHub Actions hoặc similar automation platforms có thể trigger LangSmith evaluation runs và fail build nếu pass rate drop below acceptable threshold (e.g., 95%). Approach này catch regressions early, khi cost của fixing còn thấp.

Notion setup CI/CD cho prompts như sau: Prompt templates stored trong Git repo, mỗi file có corresponding test file defining golden examples. Khi developer push changes, GitHub Actions trigger workflow chạy pytest test suite. Tests call LangSmith API để run evaluations, parse results, và assert pass rate meets threshold. Nếu tests pass, pull request được mark as ready for review. Nếu fail, developer receive notification với links đến failed cases trong LangSmith UI. Entire process từ commit đến results mất khoảng 3-5 phút, providing fast feedback loop.

## Advanced Evaluation Patterns

Beyond simple pass/fail checks, sophisticated evaluations có thể provide richer insights. Comparative evaluation runs cùng input qua both old và new prompt versions, generates outputs từ cả hai, và asks evaluator LLM "which output is better and why?". Approach này useful khi không có clear objective criteria—quality là somewhat subjective nhưng relative comparison easier than absolute judgment.

OpenAI research team published paper về "LLM-as-judge" evaluation năm 2023, showing rằng GPT-4 có thể reliably evaluate outputs của other LLMs khi given clear rubric. Họ so sánh GPT-4 evaluations với human expert evaluations trên dataset gồm 800 examples, tìm thấy 85% agreement rate. Disagreements thường occur ở edge cases where even human experts không consensus. Finding này validate approach dùng LLMs để automate evaluation, significantly reducing manual effort required.

Duolingo sử dụng LLM-as-judge cho lesson content generation. Khi AI generates new language exercises, evaluator agent kiểm tra chúng theo multiple dimensions: grammar correctness, vocabulary appropriateness for learner level, cultural sensitivity, engagement potential, và pedagogical effectiveness. Mỗi dimension được score từ 1-5 với explanation. Overall score là weighted average, và exercises chỉ được accept nếu score trên 4.0. Approach này filter out low-quality content automatically, ensure chỉ best materials reach learners.

Multi-agent evaluation là pattern khác gaining traction. Thay vì rely on single evaluator, use ensemble of specialized evaluators—mỗi cái focus on different aspect. Ví dụ, cho medical advice chatbot: Factual Accuracy Evaluator check medical facts against knowledge base, Safety Evaluator ensure advice không potentially harmful, Empathy Evaluator assess bedside manner, và Clarity Evaluator measure understandability for layperson. Final evaluation aggregate scores từ all evaluators, có thể với different weights based on priority.

Replicate AI implement ensemble evaluation cho text-to-image model prompts. Họ có năm evaluators: Adherence (output match input description?), Quality (image resolution và artifacts), Safety (không có NSFW content), Bias (representation diversity), và Creativity (uniqueness và artistic merit). Mỗi evaluator implemented differently—Adherence dùng CLIP model, Safety dùng classifier trained on labeled data, Bias analyze demographics của generated people, Creativity measure variance so với similar prompts. Outputs phải pass minimum threshold trên tất cả dimensions to be considered acceptable.

## Maintaining Test Suites Over Time

Golden datasets không phải "set and forget"—chúng cần maintenance như bất kỳ codebase nào. Khi product evolves, use cases mới emerge và old ones become less relevant. User expectations shift. New attack vectors discovered. Tất cả những changes này cần reflected trong test suite để ensure nó remains effective quality gate.

Quarterly review là good practice. Team cùng nhau go through golden dataset, discuss whether mỗi entry vẫn còn relevant, update expectations nếu cần, và add new entries cho use cases chưa covered. Process này cũng là opportunity để retire obsolete entries—giữ dataset lean và focused. Anthropic team schedule half-day workshop mỗi quarter dedicated to test suite maintenance, treating nó như important engineering work rather than chore.

Automated maintenance cũng possible đến certain extent. Bạn có thể track metrics về test cases—số lần mỗi case fail trong last N runs, last time expectations were updated, how often case gets manually reviewed khi failures happen. Cases consistently passing và never causing investigation có thể candidates for retirement—chúng có thể testing obvious functionality already well-covered. Conversely, cases frequently failing và requiring manual judgment có thể cần refined expectations or broken into more specific subcases.

User feedback integration vào test suite là critical loop. Mỗi khi user report issue với AI output, investigate xem có corresponding test case covering scenario đó không. Nếu không, add one. Nếu có nhưng test passed, điều đó indicate expectations trong test không accurate—update chúng để reflect actual user expectations. Slack implement automated process: serious bug reports automatically create draft test case entries, assigned to engineer fixing bug để finalize và add to suite. Approach này ensure test coverage naturally expands to cover real-world failure modes.

Version control for golden datasets cũng important. Store datasets trong Git alongside code, track changes over time. Khi expectations change, commit message nên explain reasoning. Review history của dataset có thể reveal interesting patterns—có phải expectations đang gradually relax (sign of quality degradation)? Hay đang tighten (sign of improving standards)? Are new entries mostly edge cases (sign of product maturing) hay major use cases (sign of product pivoting)?

Documentation về prompt testing strategy nên accessible cho toàn team. New engineers joining project cần hiểu why testing matters, how to run tests locally, how to interpret results, và how to modify prompts safely. Many teams create "Prompt Engineering Guidelines" document covering dos and don'ts, common pitfalls, và testing workflow. GitHub Copilot team có internal wiki với examples của good và bad prompt modifications, annotated với explanations về tại sao certain changes worked hoặc failed. Resource này invaluable for onboarding và knowledge sharing.

Cost consideration cũng factor trong test suite design. Running hundreds of test cases với GPT-4 có thể expensive, especially nếu chạy multiple times per day. Strategies để manage cost: use cheaper models cho initial screening (GPT-3.5 catches many obvious issues), implement smart test selection (only run tests likely affected by changes), và schedule comprehensive runs during off-peak hours khi API rates lower. Shopify estimate rằng thay vì chạy full 300-test suite sau mỗi commit, họ run 50-test quick suite (cost $2, takes 2 minutes) và chỉ run full suite nightly (cost $15, takes 15 minutes). Hybrid approach này balance feedback speed với cost efficiency.

Cuối cùng, nhớ rằng tests không thay thế human judgment—chúng augment nó. Automated testing excellent for catching regressions và ensuring baseline quality, nhưng cannot replace thoughtful product review và user empathy. Rachel đã học được rằng comprehensive testing framework combined with staged rollouts (deploy to 5% users first, monitor metrics, gradually expand) provide best defense against unintended consequences. Treat prompts like code không có nghĩa treat chúng exactly giống code—AI systems have unique characteristics requiring adapted practices. Nhưng core principle remains: systematic testing là foundation của reliable systems, cho dù system đó powered by traditional code hay AI models.
