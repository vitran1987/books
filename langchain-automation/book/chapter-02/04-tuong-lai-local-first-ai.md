# Tương Lai Local-First AI: Chuẩn Bị Cho Cuộc Cách Mạng SLM

Marcus Taylor cảm thấy đau đầu khi nhìn vào hóa đơn OpenAI tháng 9 năm 2024: 4,872 đô la - gấp đôi so với tháng trước và gấp ba so với dự toán ban đầu. Nền tảng giáo dục AI của anh đang phát triển tốt: từ 3,000 lên 8,000 người dùng hoạt động chỉ trong 2 tháng. Nhưng cùng với sự tăng trưởng người dùng là chi phí API cũng tăng theo - và nó tăng tuyến tính, thậm chí siêu tuyến tính vì người dùng càng tương tác nhiều, họ càng hỏi AI nhiều câu hỏi. Marcus tính toán nhanh: với xu hướng hiện tại, khi đạt 20,000 người dùng (mục tiêu cuối năm), hóa đơn OpenAI hàng tháng sẽ lên tới ~12,000 đô la - chiếm 60% doanh thu. Biên lợi nhuận gộp mà các nhà đầu tư mong muốn là 70-80%, nhưng Marcus đang hướng tới chỉ 40%. Anh biết rằng nếu không giải quyết được cấu trúc chi phí này, doanh nghiệp sẽ không bền vững - hoặc phải tăng giá đáng kể (rủi ro mất người dùng), hoặc phải huy động vốn lớn để bù đắp chi phí (làm loãng quyền sở hữu), hoặc tìm cách giảm đáng kể chi phí AI.

Trong khi đó, nền tảng LearnLocal.dev của nhà sáng lập Nina Patel - ra mắt cùng thời điểm với số lượng người dùng tương tự - chỉ tốn 890 đô la mỗi tháng cho 7,500 người dùng. Sự khác biệt đáng kinh ngạc này không phải vì Nina sử dụng ít AI hơn - thực tế, học viên của cô tương tác với AI nhiều hơn. Bí quyết của Nina: cô đang chạy 70% khối lượng công việc AI trên thiết bị của người dùng bằng cách sử dụng các mô hình ngôn ngữ nhỏ (Small Language Models - SLMs) như Llama 3.1 8B và Phi-3 mini, chỉ sử dụng GPT-4 cho những tác vụ thực sự cần khả năng của mô hình tiên tiến. Các truy vấn đơn giản như "Bài học tiếp theo là gì?", "Tóm tắt phần này", "Cho tôi một gợi ý" được xử lý bằng SLM chạy trực tiếp trong trình duyệt hoặc trên laptop của người dùng với WebGPU. Các tác vụ suy luận phức tạp như "Tạo kế hoạch học tập cá nhân hóa dựa trên phong cách học của tôi" vẫn sử dụng GPT-4 qua API. Kết quả: giảm 70% chi phí, cùng với các lợi ích bổ sung như phản hồi tức thì (không có độ trễ API), bảo mật (dữ liệu nhạy cảm không rời khỏi thiết bị), và khả năng hoạt động ngoại tuyến (học viên có thể học mà không cần kết nối internet).

Câu chuyện của Marcus và Nina đại diện cho một bước ngoặt đang diễn ra trong lĩnh vực giáo dục AI. Trong 2 năm qua, các mô hình ngôn ngữ nhỏ (SLMs) - các mô hình với 1-20 tỷ tham số được tối ưu hóa để chạy hiệu quả trên phần cứng tiêu dùng - đã cải thiện chất lượng một cách đáng kể. Llama 3.1 8B của Meta, Phi-3 của Microsoft, Gemma 2 của Google, và Mistral 7B đều mang lại hiệu suất tương đương với GPT-3.5 cho nhiều tác vụ phổ biến trong giáo dục: trả lời câu hỏi, tóm tắt, giải thích đơn giản, đoạn mã lập trình, dịch thuật. Quan trọng hơn, các mô hình này có thể chạy cục bộ - trên laptop, trong trình duyệt, thậm chí trên thiết bị di động - với chi phí gần như bằng không sau khi thiết lập ban đầu. Theo báo cáo State of Open Source AI 2024, 34% ứng dụng AI đã tích hợp các mô hình cục bộ cho ít nhất một phần khối lượng công việc, tăng từ chỉ 8% vào năm 2023. Xu hướng này chỉ tăng tốc khi phần cứng hỗ trợ (như Apple Silicon với Neural Engine, Intel Arc GPU, AMD RDNA) và các khung phần mềm (WebGPU, ONNX Runtime, MLX) trở nên hoàn thiện hơn.

## Small Language Models: Nhỏ Nhưng Hiệu Quả

SLMs không phải là "phiên bản kém thông minh" của các mô hình lớn (LLMs) - chúng là các mô hình được tối ưu hóa đặc biệt cho hiệu quả mà vẫn duy trì chất lượng cao trong các lĩnh vực cụ thể. Ví dụ, Llama 3.1 8B có 8 tỷ tham số - nhỏ hơn 22 lần so với GPT-4 (ước tính 175 tỷ tham số) - nhưng với việc tinh chỉnh trên dữ liệu giáo dục cụ thể, nó có thể đạt hoặc thậm chí vượt qua chất lượng của GPT-3.5 trong các câu hỏi và trả lời giáo dục. Phi-3 mini chỉ có 3.8 tỷ tham số nhưng vượt trội hơn các mô hình lớn gấp đôi nhờ phương pháp đào tạo sáng tạo sử dụng dữ liệu tổng hợp chất lượng cao. Gemma 2 9B cân bằng giữa kích thước và khả năng, mang lại khả năng suy luận ấn tượng với footprint đủ nhỏ để chạy trên các laptop chơi game thông thường. Điều quan trọng: tất cả các mô hình này đều mã nguồn mở và miễn phí sử dụng cho mục đích thương mại - không có phí tính theo token như các API đám mây.

Yêu cầu phần cứng để chạy SLMs cục bộ đã giảm đáng kể trong 2 năm qua nhờ các kỹ thuật lượng tử hóa. Lượng tử hóa là quá trình chuyển đổi trọng số mô hình từ các số dấu phẩy động 16-bit hoặc 32-bit xuống các số nguyên 4-bit hoặc 8-bit - giảm kích thước mô hình 4-8 lần với tổn thất chất lượng tối thiểu. Llama 3.1 8B ở định dạng full precision (FP16) cần ~16GB RAM, nhưng ở lượng tử hóa 4-bit chỉ cần ~4.5GB - có thể chạy trên bất kỳ laptop hiện đại nào kể cả MacBook Air bản cơ bản. Phi-3 mini lượng tử hóa chỉ cần 2GB, có thể chạy trong trình duyệt với WebGPU trên các thiết bị từ năm 2020 trở lại đây. Tốc độ suy luận cũng ấn tượng: trên chip Apple M2, Llama 3.1 8B lượng tử hóa tạo ra ~25 token mỗi giây - đủ nhanh cho các trải nghiệm hội thoại thời gian thực. Trên các GPU chuyên dụng như RTX 4070 hoặc AMD 7800 XT, tốc độ có thể lên tới 80-100 token mỗi giây - nhanh hơn cả các cuộc gọi API đến LLMs đám mây khi tính cả độ trễ mạng.

Nina Patel từ LearnLocal.dev chia sẻ chi tiết triển khai hệ thống SLM cục bộ. Cô sử dụng transformers.js - thư viện JavaScript để chạy các mô hình transformer trực tiếp trong trình duyệt với tăng tốc WebGPU. Khi học viên đăng nhập lần đầu, trình duyệt tải xuống mô hình lượng tử hóa (4.5GB cho Llama 3.1 8B) - tương đương kích thước của một bộ phim HD hoặc một bản cập nhật trò chơi lớn, chấp nhận được với tốc độ internet hiện đại. Mô hình được lưu trữ cục bộ và tái sử dụng cho tất cả các phiên sau. Mỗi khi học viên hỏi một câu hỏi đơn giản, truy vấn được xử lý cục bộ bằng transformers.js với thời gian phản hồi 200-500ms - nhanh hơn các cuộc gọi API. Học viên có thể tiếp tục học hoàn toàn ngoại tuyến sau khi tải xuống ban đầu. Những học viên nhạy cảm về quyền riêng tư đánh giá cao rằng câu hỏi và dữ liệu học tập của họ không rời khỏi thiết bị. Nina ước tính rằng với cơ sở người dùng hiện tại, cách tiếp cận local-first giúp cô tiết kiệm ~3,000 đô la mỗi tháng so với cách tiếp cận API đám mây thuần túy - và khoản tiết kiệm này tăng tuyến tính theo sự tăng trưởng người dùng.

Việc chọn mô hình SLM phù hợp cho trường hợp sử dụng giáo dục đòi hỏi phải hiểu rõ các yếu tố đánh đổi. Llama 3.1 8B xuất sắc trong việc cung cấp kiến thức chung, suy luận và theo dõi hướng dẫn - là sự lựa chọn mặc định tốt. Phi-3 mini nhỏ hơn và nhanh hơn nhưng chất lượng hơi thấp hơn - tốt cho các câu hỏi và tóm tắt thông tin đơn giản. Mistral 7B mạnh mẽ trong các tác vụ lập trình - lý tưởng nếu bạn đang dạy lập trình. Các mô hình Gemma 2 có nhiều kích thước (2B, 9B, 27B) cho phép lựa chọn linh hoạt dựa trên phần cứng mục tiêu. Khuyến nghị cho các nhà sáng lập đơn lẻ: bắt đầu với Llama 3.1 8B được lượng tử hóa thành 4-bit - sự cân bằng tốt nhất giữa chất lượng và khả năng tiếp cận. Kiểm tra hiệu suất trên các thiết bị người dùng đại diện (đừng chỉ kiểm tra trên laptop phát triển cao cấp). Thu thập số liệu: chất lượng phản hồi, độ trễ, mức sử dụng bộ nhớ, và sự hài lòng của người dùng. Lặp lại dựa trên dữ liệu - có thể phát hiện ra rằng mô hình nhỏ hơn đủ cho phần lớn các truy vấn, dành mô hình lớn hơn cho các trường hợp phức tạp.

## Kiến Trúc Lai: Đám Mây + Cục Bộ

Cách tiếp cận chỉ cục bộ có những hạn chế rõ ràng. Các mô hình cục bộ, dù ấn tượng, vẫn không thể sánh với các mô hình đám mây tiên tiến như GPT-4 cho các tác vụ suy luận phức tạp, kiến thức chung rộng rãi, hoặc hiểu biết tinh tế. Sự phân mảnh phần cứng có nghĩa là không phải người dùng nào cũng có thiết bị đủ khả năng - học viên trên các laptop cấu hình thấp hoặc điện thoại cũ không thể chạy các mô hình cục bộ một cách mượt mà. Khả năng hoạt động ngoại tuyến là một điểm cộng nhưng nhiều tính năng vẫn yêu cầu kết nối đám mây (đồng bộ tiến độ, truy cập nội dung cập nhật, các tính năng xã hội). Giải pháp không phải là chọn cục bộ HOẶC đám mây mà là kết hợp thông minh cả hai trong một kiến trúc lai tận dụng sức mạnh của từng phương pháp.

Mô hình kiến trúc lai cho giáo dục AI: mặc định sử dụng SLM cục bộ cho phần lớn các tương tác, chọn lọc nâng cao lên LLM đám mây cho các tác vụ yêu cầu khả năng vượt trội. Quyết định về việc khi nào sử dụng cục bộ và khi nào sử dụng đám mây có thể dựa trên quy tắc hoặc thông minh. Cách tiếp cận dựa trên quy tắc đơn giản: các truy vấn đơn giản ("Nội dung bài học 5 là gì?", "Định nghĩa đệ quy") sẽ được gửi đến SLM cục bộ; các truy vấn phức tạp ("Thiết kế sơ đồ cơ sở dữ liệu cho...") sẽ được gửi đến LLM đám mây. Cách tiếp cận thông minh sử dụng một bộ phân loại - có thể là một mô hình nhỏ khác hoặc các quy tắc đơn giản - để dự đoán độ phức tạp của truy vấn và định tuyến tương ứng. Ví dụ: các truy vấn dưới 10 từ thường là đơn giản → cục bộ; các truy vấn chứa "giải thích chi tiết", "phân tích", "so sánh" → đám mây; các truy vấn về nội dung đã học gần đây → cục bộ; các truy vấn về các chủ đề nâng cao → đám mây.

LearnLocal.dev triển khai một hệ thống lai tinh tế với cơ chế dự phòng. Đường dẫn chính: mọi truy vấn đều được gửi đến SLM cục bộ trước với thời gian chờ 3 giây. Nếu SLM cục bộ không tạo ra phản hồi thỏa đáng (được định nghĩa bởi điểm tin cậy hoặc độ dài phản hồi) hoặc hết thời gian, tự động dự phòng sang API đám mây. Từ góc độ người dùng, trải nghiệm là liền mạch - họ không biết đằng sau truy vấn được xử lý bởi mô hình cục bộ hay đám mây, chỉ thấy một phản hồi AI xuất hiện. Theo thời gian, Nina thu thập số liệu về các loại truy vấn nào thường dự phòng sang đám mây và sử dụng dữ liệu này để tinh chỉnh mô hình cục bộ đặc biệt cho những trường hợp đó - dần dần giảm sự phụ thuộc vào đám mây. Triển khai ban đầu: 55% truy vấn được xử lý cục bộ. Sau 3 tháng tinh chỉnh: 73% truy vấn được xử lý cục bộ. Dự đoán: 80-85% phạm vi xử lý cục bộ trong 6 tháng. Tiết kiệm chi phí tích lũy theo thời gian khi mô hình cục bộ trở nên tốt hơn.

RAG với các mô hình cục bộ thêm vào sự phức tạp thú vị. Các embedding nội dung khóa học vẫn có thể được lưu trữ trong cơ sở dữ liệu đám mây (Supabase) - bản thân các embedding thì nhỏ và không nhạy cảm. Khi người dùng truy vấn, có hai tùy chọn: (1) Truy xuất xảy ra ở phía máy chủ, các đoạn được truy xuất gửi đến khách hàng, SLM cục bộ tạo phản hồi sử dụng các đoạn đó; (2) Các embedding được đồng bộ hóa với thiết bị cục bộ, truy xuất xảy ra ở phía khách hàng với thư viện tìm kiếm vector cục bộ như hnswlib.js, sau đó SLM cục bộ tạo phản hồi. Tùy chọn 1 đơn giản hơn nhưng vẫn yêu cầu gọi mạng để truy xuất. Tùy chọn 2 thực sự có khả năng hoạt động ngoại tuyến nhưng yêu cầu đồng bộ hóa dữ liệu đáng kể (các embedding cho toàn bộ thư viện khóa học có thể 50-200MB). Nina chọn tùy chọn 1 cho hiện tại - là một sự thỏa hiệp hợp lý. Kế hoạch trong tương lai: tùy chọn 2 cho "chế độ ngoại tuyến" mà học viên có thể bật nếu họ dự đoán làm việc ngoại tuyến (trên máy bay, khu vực h remote).

## Kiến Trúc Đảm Bảo Tương Lai: Thiết Kế Để Thay Thế Mô Hình

Kiến trúc quan trọng nhất mà một nhà sáng lập đơn lẻ cần triển khai không phải là chọn mô hình "đúng" hôm nay mà là thiết kế hệ thống cho phép thay thế dễ dàng các mô hình trong tương lai. Cảnh quan AI thay đổi cực kỳ nhanh chóng: các mô hình tốt hơn được phát hành mỗi tháng, giá cả thay đổi, khả năng tiến hóa, và kỳ vọng của người dùng cũng thay đổi. Hệ thống cứng nhắc với các phụ thuộc mã hóa cứng vào các API mô hình cụ thể sẽ trở thành nợ kỹ thuật nhanh chóng. Kiến trúc linh hoạt cho phép thử nghiệm với các mô hình khác nhau, chuyển sang các lựa chọn tốt hơn, và thích ứng với các thay đổi về kinh tế - điều này rất quan trọng cho sự bền vững lâu dài của doanh nghiệp sử dụng AI.

Mô hình lớp trừu tượng là kỹ thuật cốt lõi. Thay vì gọi API OpenAI trực tiếp từ mã ứng dụng, hãy tạo một lớp trừu tượng mỏng (có thể đơn giản như một lớp Python hoặc module JS) để lộ giao diện tổng quát: `generateResponse(prompt, context, parameters)`. Ở dưới cùng, lớp trừu tượng này có thể định tuyến đến OpenAI, Anthropic, Llama cục bộ, hoặc bất kỳ mô hình nào khác. Việc chuyển đổi mô hình chỉ yêu cầu thay đổi triển khai trong lớp trừu tượng, không cần phải chạm vào logic kinh doanh trong toàn bộ ứng dụng. LangChain cung cấp điều này ngay lập tức với lớp trừu tượng `ChatModel` và các triển khai cụ thể cho hàng chục nhà cung cấp. Việc sử dụng LangChain không chỉ là về việc tận dụng các chuỗi và tác nhân hiện có - mà còn về việc đảm bảo tương lai qua các giao diện tiêu chuẩn hóa.

Lựa chọn mô hình dựa trên cấu hình đưa ra một bước xa hơn cho lớp trừu tượng. Lưu trữ các lựa chọn mô hình trong các tệp cấu hình hoặc biến môi trường thay vì mã hóa cứng. Ví dụ cấu hình: `DEFAULT_MODEL=gpt-4`, `SIMPLE_QUERY_MODEL=llama-3-8b-local`, `CODE_GEN_MODEL=claude-3.5`, `FALLBACK_MODEL=gpt-3.5-turbo`. Mã ứng dụng đọc cấu hình và khởi tạo các mô hình thích hợp. Thay đổi mô hình chỉ yêu cầu cập nhật cấu hình, không thay đổi mã. Càng tốt hơn: sở thích mô hình theo người dùng - có thể có các bậc với các mô hình khác nhau (bậc miễn phí sử dụng các mô hình cục bộ/rẻ tiền, bậc cao cấp sử dụng GPT-4), hoặc cho phép người dùng chọn mô hình dựa trên phần cứng của họ (tự động phát hiện khả năng thiết bị và gợi ý mô hình cục bộ phù hợp). CodeMentor.ai đã triển khai điều này với thành công lớn: người dùng miễn phí nhận được Llama 3 8B cục bộ + GPT-3.5 dự phòng, người dùng pro nhận được GPT-4, người dùng doanh nghiệp nhận được Claude Opus. Cùng một mã nguồn hỗ trợ tất cả các bậc thông qua cấu hình.

Hạ tầng kiểm tra là rất quan trọng khi làm việc với nhiều mô hình. Xây dựng bộ đánh giá với các trường hợp thử nghiệm đại diện bao gồm các loại truy vấn khác nhau, độ khó, và lĩnh vực. Chạy bộ này trên các mô hình khác nhau định kỳ để đo lường chất lượng, độ trễ, và chi phí. Khi có mô hình mới được phát hành (như Llama 3.2 hay GPT-5), chạy bộ đánh giá để so sánh khách quan với các mô hình hiện tại. Quyết định dựa trên dữ liệu thì tốt hơn trực giác - có thể phát hiện ra rằng mô hình mới không thực sự tốt hơn cho trường hợp sử dụng cụ thể, hoặc tốt hơn đáng kể khiến việc chuyển đổi trở nên rõ ràng. Nền tảng QuickLearn.io duy trì tập dữ liệu đánh giá với 500 truy vấn giáo dục được chọn lọc qua các cấp độ độ khó. Hàng tháng họ đo lường tất cả các mô hình có sẵn (đám mây và cục bộ), tạo báo cáo so sánh toàn diện hướng dẫn quyết định lựa chọn mô hình. Đầu tư 4-5 giờ mỗi tháng cho việc kiểm tra tiết kiệm hàng ngàn đô la trong việc tối ưu hóa lựa chọn mô hình.

## Chuẩn Bị Cho Làn Sóng Tiếp Theo: Phần Cứng AI Trên Thiết Bị

Cuối năm 2024 và đầu năm 2025 sẽ chứng kiến những thay đổi lớn trong phần cứng AI trên thiết bị. Apple đã công bố chip M4 với Neural Engine được nâng cấp đáng kể - ước tính nhanh hơn 3 lần so với M3 cho các khối lượng công việc AI. Các chip Snapdragon X Elite của Qualcomm cho PC Windows bao gồm các NPUs (Đơn vị Xử lý Đệ quy) mạnh mẽ có khả năng chạy các mô hình 13B tham số cục bộ. AMD và Intel đang cạnh tranh quyết liệt với các công nghệ tăng tốc AI tương ứng. Các điện thoại thông minh đang trở nên có khả năng AI: iPhone 16 Pro với chip A18 có thể chạy Phi-3 mini một cách mượt mà, các điện thoại Android cao cấp với Snapdragon 8 Gen 4 cũng tương đương. Các trình duyệt đang tiến hóa: việc áp dụng WebGPU đang tăng trưởng, WebNN (API Mạng Nơron Web) đang được phát triển hứa hẹn cách chuẩn hóa để tận dụng các bộ tăng tốc ML trên thiết bị. Tất cả những phát triển này chỉ ra một tương lai mà việc chạy các mô hình AI tinh vi trên thiết bị cục bộ trở thành điều bình thường hơn là ngoại lệ.

Các nhà sáng lập đơn lẻ xây dựng các nền tảng giáo dục AI hôm nay cần thiết kế kiến trúc dự đoán điều này. Có nghĩa là: không tối ưu hóa quá mức chỉ cho các API đám mây với giả định rằng điều đó sẽ kéo dài mãi mãi; xây dựng với giả định rằng một phần đáng kể của cơ sở người dùng sẽ có thiết bị đủ khả năng trong 1-2 năm tới; đầu tư vào việc tìm hiểu các công nghệ AI cục bộ ngay bây giờ khi chúng còn đang mới nổi thay vì chờ đợi đến khi trưởng thành; cấu trúc dữ liệu và quy trình làm việc để hỗ trợ cả xử lý AI tập trung (đám mây) và phân tán (cục bộ). Các hành động cụ thể: thử nghiệm với việc chạy SLMs cục bộ ngay cả khi chưa triển khai vào sản xuất, theo dõi các xu hướng phần cứng và cải tiến mô hình, tham gia vào các cộng đồng xung quanh transformers.js / ONNX / WebGPU để cập nhật kiến thức, và quan trọng nhất là kiến trúc hệ thống với sự phân tách rõ ràng giữa "xử lý AI" và "logic kinh doanh" để việc thay thế backend AI không làm gián đoạn hoạt động kinh doanh.

Đặt cược vào AI ưu tiên cục bộ không có nghĩa là từ bỏ AI đám mây - các mô hình đám mây sẽ tiếp tục có vai trò trong tương lai gần cho các khả năng cao cấp, đào tạo tập trung, và phục vụ người dùng trên các thiết bị không đủ khả năng. Đặt cược thực sự là vào cách tiếp cận lai: sự kết hợp thông minh giữa AI cục bộ và đám mây mang lại sự kết hợp tối ưu giữa chi phí, bảo mật, hiệu suất, và khả năng. Nina Patel tóm tắt tốt: "Năm 2024 chúng ta bắt đầu với 100% đám mây, đến cuối năm ở mức 70% cục bộ 30% đám mây, năm 2025 mục tiêu 85% cục bộ. Nhưng 15% đám mây đó xử lý những tương tác quý giá và phức tạp nhất, nơi mà chất lượng quan trọng nhất. Đó là 15% mà tôi sẵn sàng trả giá cao." Đây là tư duy chiến thắng: không phải loại bỏ các API đám mây mà là phân bổ chiến lược các khối lượng công việc để tối đa hóa giá trị trên mỗi đô la chi tiêu.

