# 7.3 Tư Duy Phê Phán: Dạy Kỹ Năng Xác Minh Thông Tin Trong Thời Đại AI

Một sự cố bất ngờ trong lớp học của cô giáo Jessica Patel tại Boston đã trở thành hồi chuông cảnh tỉnh về hiểm họa của việc tin tưởng tuyệt đối vào trí tuệ nhân tạo. Marcus, học sinh lớp 6, đã nộp một bài báo cáo lịch sử về thành phố Boston, trong đó có đoạn mô tả “Vụ cháy lớn Boston năm 1822 phá hủy 70% thành phố”. Câu chuyện nghe rất thuyết phục, với nhiều con số và chi tiết cụ thể về số người thiệt mạng, nhà cửa bị tàn phá. Nhưng sự thật là: sự kiện này chưa từng xảy ra. Vụ cháy lớn thực sự ở Boston diễn ra vào năm 1872, không phải 1822, và mức độ thiệt hại cũng hoàn toàn khác biệt. Khi cô Patel hỏi Marcus lấy thông tin từ đâu, cậu bé thật thà đáp: “Em hỏi ChatGPT, nó trả lời như vậy. Em nghĩ AI thì chắc chắn đúng.” Đây chính là hiện tượng mà các nhà nghiên cứu gọi là “ảo giác AI” – khi trí tuệ nhân tạo tự tin tạo ra thông tin sai lệch nghe rất hợp lý – và đó là lý do vì sao kỹ năng xác minh thông tin trở nên cấp thiết hơn bao giờ hết.

Một nghiên cứu của NewsGuard Technologies năm 2024 đã thực hiện thí nghiệm với ba hệ thống AI lớn: ChatGPT, Google Bard (nay là Gemini) và Claude, bằng cách hỏi về 100 sự kiện lịch sử đã được xác thực. Kết quả cho thấy các mô hình AI này tạo ra thông tin sai hoặc gây hiểu lầm trong 8-12% trường hợp, tùy vào từng mô hình và loại câu hỏi. Đáng lo ngại hơn, những câu trả lời sai này thường được trình bày với mức độ tự tin ngang bằng với câu trả lời đúng – không có cảnh báo, không có ngôn ngữ dè dặt, khiến người dùng rất khó phân biệt. Đối với trẻ em, những người chưa có nền tảng kiến thức vững chắc để nhận ra điểm bất hợp lý, nguy cơ này càng lớn. Một nghiên cứu tiếp theo của Stanford Internet Observatory năm 2024 phát hiện học sinh trung học cơ sở có khả năng nhận diện thông tin sai từ AI thấp hơn 3,2 lần so với thông tin sai từ các nguồn web thông thường, bởi các em có xu hướng “tin tưởng mặc định” vào AI mạnh hơn.

Vậy làm thế nào để dạy trẻ xác minh thông tin trong thời đại AI? Câu trả lời bắt đầu từ việc xây dựng một “khung xác minh” có cấu trúc mà trẻ có thể áp dụng một cách nhất quán. Giáo sư Sam Wineburg, chuyên gia giáo dục tại Đại học Stanford và là người sáng lập Stanford History Education Group, đã phát triển một khung đơn giản nhưng hiệu quả mang tên “Bốn bước và một thói quen” dành cho kỹ năng đọc hiểu thông tin số. Khung này, ban đầu dùng để đánh giá tin tức trực tuyến, đã được điều chỉnh để phù hợp với nội dung do AI tạo ra. Bốn bước gồm: (1) Dừng lại – không vội tin bất kỳ thông tin nào; (2) Tìm hiểu nguồn gốc – kiểm tra độ tin cậy của nguồn; (3) Tìm nguồn khác đáng tin cậy để đối chiếu; (4) Truy ngược lại thông tin về nguồn gốc ban đầu. “Thói quen” là luôn tự động áp dụng bốn bước này trước khi chia sẻ hoặc sử dụng bất kỳ thông tin nào.

Khi áp dụng khung này cho AI, nó trở thành một danh sách kiểm tra thực tế mà trẻ có thể làm theo. Bước đầu tiên – “Dừng lại” – nghĩa là không bao giờ chấp nhận ngay câu trả lời đầu tiên của AI, mà nên tự hỏi: “Làm sao mình có thể kiểm chứng điều này?” hoặc “AI dựa vào đâu để trả lời như vậy?” Bước thứ hai – “Tìm hiểu nguồn gốc” – với AI sẽ khác một chút so với nội dung trên web, vì AI không có nguồn truyền thống. Trẻ có thể hỏi AI: “Thông tin này lấy từ loại nguồn nào? Có nghiên cứu hoặc tài liệu cụ thể không?” Nếu AI không thể cung cấp nguồn cụ thể, đó là dấu hiệu cảnh báo lớn. Bước thứ ba – “Tìm nguồn khác” – yêu cầu trẻ chủ động tìm kiếm thông tin từ ít nhất hai nguồn đáng tin cậy khác (như trang giáo dục, tài liệu học thuật, báo chí uy tín) để đối chiếu. Bước thứ tư – “Truy ngược lại” – nghĩa là nếu AI nhắc đến một nghiên cứu hay số liệu, trẻ cần tìm nguồn gốc thực sự của thông tin đó để xác minh và tránh bị bóp méo.

Một ví dụ thành công đến từ Singapore cho thấy khung này có thể được áp dụng hiệu quả ra sao. Trường Trung học Ngee Ann đã triển khai chương trình “Phòng thí nghiệm kiểm chứng AI” cho học sinh lớp 7-8 từ năm 2023. Mỗi tuần, học sinh được giao nhiệm vụ lấy một thông tin bất kỳ từ AI về các chủ đề như khoa học, lịch sử, thời sự, sau đó dành 45 phút để kiểm chứng hoặc bác bỏ thông tin đó bằng nhiều nguồn khác nhau. Các em ghi lại quá trình kiểm tra trong một “báo cáo kiểm chứng” và trình bày kết quả trước lớp. Sau một học kỳ, đánh giá cho thấy 87% học sinh có thể nhận diện và giải thích ít nhất một vấn đề với nội dung do AI tạo ra khi được kiểm tra với các tình huống mới, so với chỉ 34% trước chương trình. Quan trọng hơn, thái độ của các em đã thay đổi: các em phát triển được “tư duy hoài nghi lành mạnh” – không phải sợ hãi AI, mà là biết “tin nhưng phải kiểm chứng”.

Một công cụ thực tiễn khác giúp trẻ nhận diện rủi ro từ AI là dạy các em về “dấu hiệu cảnh báo” – những tín hiệu cho thấy câu trả lời của AI cần được kiểm tra kỹ lưỡng hơn. Theo tiến sĩ Chirag Shah, chuyên gia về khoa học thông tin tại Đại học Washington, có năm dấu hiệu chính mà ngay cả trẻ nhỏ cũng có thể học để nhận biết. Dấu hiệu thứ nhất: “Quá chi tiết không cần thiết” – khi AI đưa ra những con số cực kỳ cụ thể (ví dụ “chính xác 1.247 người”) về các sự kiện lịch sử mà không có nguồn dẫn chứng, đó thường là dấu hiệu AI đang “bịa” chi tiết để nghe cho thuyết phục. Dấu hiệu thứ hai: “Thiếu sắc thái” – thế giới thực hiếm khi chỉ có đúng hoặc sai, nên nếu AI đưa ra một câu trả lời tuyệt đối về một chủ đề gây tranh cãi mà không đề cập đến các góc nhìn khác, đó là tín hiệu cần cảnh giác. Dấu hiệu thứ ba: “Ngày tháng và tên không nhất quán” – nếu hỏi AI cùng một câu nhiều lần mà nhận được các ngày tháng hoặc tên khác nhau, chứng tỏ AI đang đoán chứ không dựa vào dữ liệu xác thực. Dấu hiệu thứ tư: “Nguồn không thể kiểm chứng” – khi AI viện dẫn một “nghiên cứu năm 2023” nhưng không thể nêu rõ tên nghiên cứu, tác giả hoặc tạp chí cụ thể. Và dấu hiệu thứ năm: “Thông tin quá tốt để là sự thật” – nếu AI đưa ra một sự thật nghe quá bất ngờ hoặc kịch tính, đó chính là lúc cần kiểm tra lại thật kỹ.

Một phương pháp dạy học hiệu quả khác là “săn lỗi cố ý”. Thầy Michael Torres, giáo viên khoa học tại Texas, đã sáng tạo một trò chơi cho lớp 8 của mình: mỗi tuần, thầy cố tình hỏi AI một câu mà thầy biết xác suất AI trả lời sai hoặc bịa là rất cao, sau đó thách thức học sinh tìm và giải thích lỗi sai đó. Ví dụ, thầy có thể hỏi AI về một phát hiện khoa học ít người biết hoặc về một nhân vật lịch sử không nổi tiếng. Học sinh làm việc theo nhóm để điều tra, nhóm nào phát hiện và giải thích đúng nguyên nhân AI mắc lỗi (chẳng hạn do dữ liệu huấn luyện hạn chế, nhầm lẫn giữa các sự kiện tương tự…) sẽ được thưởng điểm. Trò chơi này không chỉ rèn kỹ năng kiểm chứng thông tin mà còn giúp học sinh hiểu sâu hơn về cách AI hoạt động và giới hạn của nó. Sau một năm, thầy Torres nhận thấy học sinh của mình đã phát triển được “trực giác AI” – khả năng cảm nhận khi nào câu trả lời của AI đáng tin và khi nào cần kiểm tra thêm, dựa trên các mẫu lỗi mà các em từng gặp.

Với trẻ nhỏ hơn (8-10 tuổi), quá trình xác minh cần được đơn giản hóa nhưng vẫn giữ nguyên tắc cốt lõi. Thay vì khung phức tạp, có thể dạy quy tắc “hai nguồn”: bất kỳ thông tin nào từ AI cũng phải được xác nhận bởi ít nhất hai nguồn khác trước khi coi là đúng. Một trong hai nguồn nên là sách giáo khoa, bách khoa toàn thư hoặc trang web giáo dục uy tín như Khan Academy hay National Geographic Kids. Cô Carol Henderson, thủ thư kiêm điều phối viên công nghệ tại một trường tiểu học ở Oregon, đã áp dụng quy tắc này cho học sinh lớp 4-5. Cô tạo ra “Bảng kiểm xác minh” đơn giản: (1) AI nói điều này, (2) Em tìm thấy điều này ở nguồn 1, (3) Em tìm thấy điều này ở nguồn 2, (4) Cả ba nguồn có đồng nhất không, hoặc điểm khác biệt là gì. Sau vài tháng, việc xác minh đã trở thành thói quen tự động. Một học sinh lớp 5 chia sẻ: “Bây giờ con thấy lạ nếu chỉ tin một nguồn. Con luôn muốn kiểm tra thêm ít nhất một chỗ nữa.”

Một khía cạnh quan trọng khác của tư duy phản biện với AI là dạy trẻ về mức độ tự tin và xác suất trong câu trả lời. AI thường trả lời với vẻ rất chắc chắn ngay cả khi mức độ không chắc chắn cao. Dạy trẻ nhận biết và đặt câu hỏi về mức độ tự tin này là điều thiết yếu. Tiến sĩ Devi Parikh, nhà nghiên cứu AI tại Georgia Tech, khuyến khích dạy trẻ hỏi AI: “Bạn có chắc chắn về thông tin này không? Nếu có, mức độ chắc chắn là bao nhiêu phần trăm?” Nhiều mô hình AI, khi được hỏi đúng cách, có thể thể hiện sự không chắc chắn. Ví dụ, thay vì khẳng định một sự thật, AI có thể nói: “Theo kiến thức phổ biến, X có khả năng đúng, nhưng tôi không có dữ liệu thời gian thực để xác nhận 100%.” Dạy trẻ nhận biết và khai thác các tín hiệu không chắc chắn này giúp các em hiểu sâu hơn về giới hạn của tri thức AI.

Cuối cùng, một công cụ vô cùng mạnh mẽ là dạy trẻ trở thành “người hoài nghi AI” thông qua trải nghiệm cá nhân với các lỗi của AI. Thay vì che giấu hoặc né tránh những lần AI sai, hãy biến chúng thành cơ hội học tập. Có thể tạo một “Bảng Oops AI” trong lớp hoặc ở nhà, nơi trẻ chia sẻ những lần phát hiện AI mắc lỗi, giải thích lỗi đó là gì và quan trọng nhất – làm thế nào các em phát hiện ra. Việc tôn vinh khả năng phát hiện lỗi, thay vì xấu hổ vì từng tin nhầm AI, sẽ tạo ra một môi trường nơi tư duy phản biện được đề cao. Một nghiên cứu của Đại học Michigan năm 2024 theo dõi 150 học sinh trong một năm học cho thấy, những em được khuyến khích “ăn mừng sự hoài nghi” phát triển kỹ năng tư duy phản biện mạnh hơn 2,1 lần so với nhóm đối chứng, theo các bài kiểm tra chuẩn hóa về năng lực đánh giá thông tin. Các em cũng tự tin hơn khi đánh giá thông tin từ bất kỳ nguồn nào, không chỉ riêng AI.

